{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN import SimpleNN\n",
    "from LR_pt import LogisticRegression\n",
    "import torch\n",
    "import os as os\n",
    "os.chdir(\"/Users/karl/Desktop/Fairness/algorithmic-fairness-exam/src\")\n",
    "from data_loader import data_loader, preprocess\n",
    "from models_clean import one_hot_cols, features\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "import matplotlib\n",
    "from PCA import fair_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = True\n",
    "\n",
    "num_samples = 1_000_000\n",
    "df = data_loader(one_hot_cols, num=num_samples)\n",
    "\n",
    "# filter columns to only include columns in the features list below\n",
    "if 'race_ethnicity' in features:\n",
    "       # remove  'race_ethnicity' from features\n",
    "       features.remove('race_ethnicity')\n",
    "else: \n",
    "       print(\"'race_ethnicity' not in features\")\n",
    "x_train, x_val, x_test, y_train,y_val , y_test, train_groups, val_groups,test_groups = preprocess(df, features, one_hot_cols)\n",
    "print(f'All rows in train_groups sum to 1: {np.allclose(np.sum(train_groups, axis=1), 1)}')\n",
    "\n",
    "# convert y_train, y_val, y_test to numpy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/NN_pca:False_E:20_lr:0.001_bs:512.pt\"\n",
    "num_classes = 2\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "# Initialize the model\n",
    "model = SimpleNN(x_train.shape[1], num_classes).to(device)\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the model predictions\n",
    "    outputs = model(x_test_tensor)\n",
    "    \n",
    "    # Convert the predictions to probabilities using softmax\n",
    "    probabilities, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "    \n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n",
    "    print(classification_report(y_test_tensor.cpu().numpy(), predicted.cpu().numpy()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have this directory fullpath = \"/Users/karl/Desktop/Fairness/algorithmic-fairness-exam/models\" where the statedict of 5 pytorch models are saved, nothing else. The models that start with \"NN\" are of the architechture of the imported class SimpleNN, models with the \"LRmodel\" have the architechture of LogisticRegression. I want a function that load these models into seperate model variable (with suitable names), and returns them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(input_size, num_classes, models_directory):\n",
    "    models = {}\n",
    "\n",
    "    for filename in os.listdir(models_directory):\n",
    "        if filename.endswith('.pt'):  # if the file is a PyTorch model\n",
    "            full_path = os.path.join(models_directory, filename)\n",
    "            state_dict = torch.load(full_path)\n",
    "\n",
    "            if filename.startswith('NN'):\n",
    "                model_name = \"NN\"\n",
    "                if \"pca:True\" in filename:\n",
    "                    model = SimpleNN(7, num_classes)  \n",
    "                    model_name += \"_FairPCA\"\n",
    "                else:\n",
    "                    model = SimpleNN(input_size, num_classes)  \n",
    "            elif filename.startswith('LRmodel'):\n",
    "                model = LogisticRegression(input_size)\n",
    "                model_name = \"LR\"\n",
    "                if \"F:NO l2\" in filename:\n",
    "                    model_name += \"_L2\"\n",
    "                elif \"F:True\" in filename:\n",
    "                    model_name += \"_FairLoss\"\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "            models[model_name] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(x_train.shape[1]-2, 2, '/Users/karl/Desktop/Fairness/algorithmic-fairness-exam/models')\n",
    "print(models)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, x_train, x_test, y_test, test_groups):\n",
    "    plt.style.use('bmh')\n",
    "    matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "    # list of metric names\n",
    "    metric_names = ['weighted_f1', 'f1_score_0', 'f1_score_1', 'mean_equalized_odds_difference']\n",
    "\n",
    "    # dictionary to hold metric values for each model\n",
    "    metric_values = {name: [] for name in metric_names}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print()\n",
    "        print(f'Evaluating {model_name}...')\n",
    "        print(\"_____________________________\\n\")\n",
    "        if model_name == 'NN_FairPCA':\n",
    "            X_fair_PCA, U, explained_variance = fair_PCA(x_train, n_components=x_train.shape[1], groups=train_groups)\n",
    "            x_train = X_fair_PCA\n",
    "            x_test_pca = x_test @ U\n",
    "\n",
    "        # get model predictions\n",
    "        if model_name.startswith('NN'):\n",
    "            # argmax of softmax output is the predicted class\n",
    "            if model_name == 'NN_FairPCA':\n",
    "                test_preds = model(x_test_pca).detach().numpy().argmax(axis=1)\n",
    "            else:\n",
    "                test_preds = model(x_test).detach().numpy().argmax(axis=1)\n",
    "        else:\n",
    "            # logistic regression outputs probabilities\n",
    "            test_preds = model(x_test).detach().numpy() > 0.5\n",
    "        print(test_preds[:10])\n",
    "        # calculate weighted f1 score\n",
    "        weighted_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "        metric_values['weighted_f1'].append(weighted_f1)\n",
    "\n",
    "        # calculate f1 score for class 0\n",
    "        f1_score_0 = f1_score(y_test, test_preds, pos_label=0)\n",
    "        metric_values['f1_score_0'].append(f1_score_0)\n",
    "\n",
    "        # calculate f1 score for class 1\n",
    "        f1_score_1 = f1_score(y_test, test_preds, pos_label=1)\n",
    "        metric_values['f1_score_1'].append(f1_score_1)\n",
    "\n",
    "        # calculate mean equalized odds difference\n",
    "        one_hot_cols = list(range(6))\n",
    "        mean_eod = np.mean([\n",
    "            equalized_odds_difference(y_test, test_preds, sensitive_features=test_groups[:, one_hot_cols.index(col)])\n",
    "            for col in one_hot_cols\n",
    "        ])\n",
    "        metric_values['mean_equalized_odds_difference'].append(mean_eod)\n",
    "        print(f'Weighted F1: {weighted_f1:.4f}')\n",
    "        print(f'F1 score (class 0): {f1_score_0:.4f}')\n",
    "        print(f'F1 score (class 1): {f1_score_1:.4f}')\n",
    "        print(f'Mean equalized odds difference: {mean_eod:.4f}')\n",
    "    \n",
    "    # create bar chart\n",
    "    x = np.arange(len(metric_names))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, model_name in enumerate(models.keys()):\n",
    "        ax.bar(x - width/2 + i*width, metric_values[model_name], width, label=model_name)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by model and metric')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, x_train, x_test, y_test, test_groups):\n",
    "    plt.style.use('bmh')\n",
    "    matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "    # list of metric names\n",
    "    metric_names = ['weighted_f1', 'f1_score_0', 'f1_score_1', 'mean_equalized_odds_difference']\n",
    "\n",
    "    # dictionary to hold metric values for each model\n",
    "    metric_values = {model_name: {metric: None for metric in metric_names} for model_name in models.keys()}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print()\n",
    "        print(f'Evaluating {model_name}...')\n",
    "        print(\"_____________________________\\n\")\n",
    "        if model_name == 'NN_FairPCA':\n",
    "            X_fair_PCA, U, explained_variance = fair_PCA(x_train, n_components=x_train.shape[1], groups=train_groups)\n",
    "            x_train = X_fair_PCA\n",
    "            x_test_pca = x_test @ U\n",
    "\n",
    "        # get model predictions\n",
    "        if model_name.startswith('NN'):\n",
    "            # argmax of softmax output is the predicted class\n",
    "            if model_name == 'NN_FairPCA':\n",
    "                test_preds = model(x_test_pca).detach().numpy().argmax(axis=1)\n",
    "            else:\n",
    "                test_preds = model(x_test).detach().numpy().argmax(axis=1)\n",
    "        else:\n",
    "            # logistic regression outputs probabilities\n",
    "            test_preds = model(x_test).detach().numpy() > 0.5\n",
    "        # calculate weighted f1 score\n",
    "        weighted_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "\n",
    "        # calculate f1 score for class 0\n",
    "        f1_score_0 = f1_score(y_test, test_preds, pos_label=0)\n",
    "\n",
    "        # calculate f1 score for class 1\n",
    "        f1_score_1 = f1_score(y_test, test_preds, pos_label=1)\n",
    "\n",
    "        # calculate mean equalized odds difference\n",
    "        one_hot_cols = list(range(6))\n",
    "        mean_eod = np.mean([\n",
    "            equalized_odds_difference(y_test, test_preds, sensitive_features=test_groups[:, one_hot_cols.index(col)])\n",
    "            for col in one_hot_cols\n",
    "        ])\n",
    "\n",
    "        # calculate weighted f1 score and other metrics\n",
    "        metric_values[model_name]['weighted_f1'] = weighted_f1\n",
    "        metric_values[model_name]['f1_score_0'] = f1_score_0\n",
    "        metric_values[model_name]['f1_score_1'] = f1_score_1\n",
    "        metric_values[model_name]['mean_equalized_odds_difference'] = mean_eod\n",
    "        print(f'Weighted F1: {weighted_f1:.4f}')\n",
    "        print(f'F1 score (class 0): {f1_score_0:.4f}')\n",
    "        print(f'F1 score (class 1): {f1_score_1:.4f}')\n",
    "        print(f'Mean equalized odds difference: {mean_eod:.4f}')\n",
    "\n",
    "    # create bar chart\n",
    "    x = np.arange(len(metric_names))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, model_name in enumerate(models.keys()):\n",
    "        scores = [metric_values[model_name][metric] for metric in metric_names]\n",
    "        ax.bar(x - width/2 + i*width, scores, width, label=model_name)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by model and metric')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(models, x_train, x_test, y_test, test_groups)\n",
    "# print types of parameters\n",
    "print(\"x_test: \", type(x_test))\n",
    "print(\"y_test: \", type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
