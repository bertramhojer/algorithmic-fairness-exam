{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN import SimpleNN\n",
    "from LR_pt import LogisticRegression\n",
    "import torch\n",
    "import os as os\n",
    "os.chdir(\"/Users/karl/Desktop/Fairness/algorithmic-fairness-exam\")\n",
    "from data_loader import data_loader, preprocess\n",
    "from models_clean import one_hot_cols, features\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from fairlearn.metrics import equalized_odds_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "processed_data.csv exists. Loading data from file.\n",
      "'race_ethnicity' not in features\n",
      "x_train: 70.00%\n",
      "x_val: 15.00%\n",
      "x_test: 15.00%\n",
      "Num features BEFORE filtering features 54\n",
      "Num features AFTER filtering features 14\n",
      "x_train shape:  (395460, 14)\n",
      "y_train shape:  (84742, 14)\n",
      "All rows in train_groups sum to 1: True\n"
     ]
    }
   ],
   "source": [
    "one_hot = True\n",
    "\n",
    "num_samples = 1_000_000\n",
    "df = data_loader(one_hot_cols, num=num_samples)\n",
    "\n",
    "# filter columns to only include columns in the features list below\n",
    "if 'race_ethnicity' in features:\n",
    "       # remove  'race_ethnicity' from features\n",
    "       features.remove('race_ethnicity')\n",
    "else: \n",
    "       print(\"'race_ethnicity' not in features\")\n",
    "x_train, x_val, x_test, y_train,y_val , y_test, train_groups, val_groups,test_groups = preprocess(df, features, one_hot_cols)\n",
    "print(f'All rows in train_groups sum to 1: {np.allclose(np.sum(train_groups, axis=1), 1)}')\n",
    "\n",
    "# convert y_train, y_val, y_test to numpy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/NN_pca:False_E:20_lr:0.001_bs:512.pt\"\n",
    "num_classes = 2\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "# Initialize the model\n",
    "model = SimpleNN(x_train.shape[1], num_classes).to(device)\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the model predictions\n",
    "    outputs = model(x_test_tensor)\n",
    "    \n",
    "    # Convert the predictions to probabilities using softmax\n",
    "    probabilities, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy())\n",
    "    \n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(y_test_tensor.cpu().numpy(), predicted.cpu().numpy(), average='weighted')\n",
    "    print(classification_report(y_test_tensor.cpu().numpy(), predicted.cpu().numpy()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have this directory fullpath = \"/Users/karl/Desktop/Fairness/algorithmic-fairness-exam/models\" where the statedict of 5 pytorch models are saved, nothing else. The models that start with \"NN\" are of the architechture of the imported class SimpleNN, models with the \"LRmodel\" have the architechture of LogisticRegression. I want a function that load these models into seperate model variable (with suitable names), and returns them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(input_size, num_classes, models_directory):\n",
    "    models = {}\n",
    "\n",
    "    for filename in os.listdir(models_directory):\n",
    "        if filename.endswith('.pt'):  # if the file is a PyTorch model\n",
    "            full_path = os.path.join(models_directory, filename)\n",
    "            state_dict = torch.load(full_path)\n",
    "\n",
    "            if filename.startswith('NN'):\n",
    "                model_name = \"NN\"\n",
    "                if \"pca:True\" in filename:\n",
    "                    model = SimpleNN(9, num_classes)  \n",
    "                    model_name += \"_FairPCA\"\n",
    "                else:\n",
    "                    model = SimpleNN(input_size, num_classes)  \n",
    "                    model_name += \"_FairLoss\"\n",
    "            elif filename.startswith('LRmodel'):\n",
    "                model = LogisticRegression(input_size)\n",
    "                model_name = \"LR\"\n",
    "                if \"F:NO l2\" in filename:\n",
    "                    model_name += \"_L2\"\n",
    "                elif \"F:True\" in filename:\n",
    "                    model_name += \"_FairLoss\"\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "            models[model_name] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=14, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models = load_models(x_train.shape[1], 2, '/Users/karl/Desktop/Fairness/algorithmic-fairness-exam/models')\n",
    "print(models['LR'])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, x_test, y_test, test_groups):\n",
    "    # list of metric names\n",
    "    metric_names = ['weighted_f1', 'f1_score_0', 'f1_score_1', 'mean_equalized_odds_difference']\n",
    "\n",
    "    # dictionary to hold metric values for each model\n",
    "    metric_values = {name: [] for name in metric_names}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # get model predictions\n",
    "        test_preds = model(x_test).detach().numpy() < 0.5\n",
    "        print(test_preds.shape)\n",
    "        print(test_preds[:10])\n",
    "        \n",
    "        # calculate weighted f1 score\n",
    "        weighted_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "        metric_values['weighted_f1'].append(weighted_f1)\n",
    "\n",
    "        # calculate f1 score for class 0\n",
    "        f1_score_0 = f1_score(y_test, test_preds, pos_label=0)\n",
    "        metric_values['f1_score_0'].append(f1_score_0)\n",
    "\n",
    "        # calculate f1 score for class 1\n",
    "        f1_score_1 = f1_score(y_test, test_preds, pos_label=1)\n",
    "        metric_values['f1_score_1'].append(f1_score_1)\n",
    "\n",
    "        # calculate mean equalized odds difference\n",
    "        one_hot_cols = list(range(6))\n",
    "        mean_eod = np.mean([\n",
    "            equalized_odds_difference(y_test, test_preds, sensitive_features=test_groups[:, one_hot_cols.index(col)])\n",
    "            for col in one_hot_cols\n",
    "        ])\n",
    "        metric_values['mean_equalized_odds_difference'].append(mean_eod)\n",
    "\n",
    "    # create bar chart\n",
    "    x = np.arange(len(metric_names))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, model_name in enumerate(models.keys()):\n",
    "        ax.bar(x - width/2 + i*width, metric_values[model_name], width, label=model_name)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by model and metric')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84742, 1)\n",
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [ True]\n",
      " [False]\n",
      " [False]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (84742x14 and 9x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_models(models, x_test, y_test, test_groups)\n\u001b[1;32m      2\u001b[0m \u001b[39m# print types of parameters\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx_test: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(x_test))\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(models, x_test, y_test, test_groups)\u001b[0m\n\u001b[1;32m      6\u001b[0m metric_values \u001b[39m=\u001b[39m {name: [] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m metric_names}\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m model_name, model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     \u001b[39m# get model predictions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     test_preds \u001b[39m=\u001b[39m model(x_test)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(test_preds\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(test_preds[:\u001b[39m10\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Fairness/algorithmic-fairness-exam/src/NN.py:25\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_input(x)\n\u001b[0;32m---> 25\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     26\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (84742x14 and 9x64)"
     ]
    }
   ],
   "source": [
    "evaluate_models(models, x_test, y_test, test_groups)\n",
    "# print types of parameters\n",
    "print(\"x_test: \", type(x_test))\n",
    "print(\"y_test: \", type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
